# COLLECTIVES OPTIMIZATION FOR CROSS REGION AI ACCELERATION

The training of Large Language Models (LLMs) is currently based on GPUs typically located in the same datacenter (DC). However, since new LLMs are getting extemely large (currently approaching trillions of parameters), training them within a single DC is not sufficient, due to limitation of computing in a single DC, and severe energy provisioning issues. Several DC operators are hence moving towards distributed training over multiple DCs interconnected by private lines or private Wide Area Networks (WAN). This project will focus on the acceleration of collective communications (CCL) over different AI regions, i.e., over different DCs (inter DC scenario). The objective of the project is twofold: on one side, it aims at refining collectives for the WAN, taking into account its heterogeneous environment (in terms of latencies, capacities, and background traffic). On the other side, it aims at developing algorithms to optimize WAN operation to support collectives, bringing the awareness of the existing traffic into the WAN to take better routing and load balancing decisions.